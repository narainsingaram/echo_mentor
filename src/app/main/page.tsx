'use client';

import { useState, useRef, useEffect, useMemo, useCallback } from 'react';
import dynamic from 'next/dynamic';
import { useSession } from "next-auth/react";
import { useRouter } from "next/navigation";

// Import new components
import PracticeModeSelector from '../components/PracticeModeSelector';
import ScenarioSelector from '../components/ScenarioSelector';
import RecordingControls from '../components/RecordingControls';
import CameraPreview from '../components/CameraPreview';
import LiveTranscriptDisplay from '../components/LiveTranscriptDisplay';
import AudioPlayback from '../components/AudioPlayback';
import FinalTranscriptDisplay from '../components/FinalTranscriptDisplay';
import DetectedPausesDisplay from '../components/DetectedPausesDisplay';
import AnalysisDisplay from '../components/AnalysisDisplay';
import LoginButton from "../components/LoginButton";


const PaceVolumeChart = dynamic(() => import('../components/PaceVolumeChart'), { ssr: false });

export default function Home() {
Â  const { data: session, status } = useSession();
Â  const router = useRouter();

  // --- ALL REACT HOOKS MUST BE DECLARED HERE, UNCONDITIONALLY ---
Â  const [transcript, setTranscript] = useState<string>('');
Â  const [liveTranscript, setLiveTranscript] = useState<string>('');
Â  const [analysis, setAnalysis] = useState<string>('');
Â  const [isAnalyzing, setIsAnalyzing] = useState<boolean>(false);
Â  const [recording, setRecording] = useState<boolean>(false);
Â  const [pauses, setPauses] = useState<{ start: number; end: number }[]>([]);
Â  const [cameraStream, setCameraStream] = useState<MediaStream | null>(null);
Â  const [audioURL, setAudioURL] = useState<string | null>(null);

Â  // New states for pace & volume
Â  const [paceData, setPaceData] = useState<{ time: number; wpm: number }[]>([]);
Â  const [volumeData, setVolumeData] = useState<{ time: number; rms: number }[]>([]);

Â  // New states for practice modes
Â  const [practiceMode, setPracticeMode] = useState<'free' | 'scenario' | null>(null);
Â  const [currentPrompt, setCurrentPrompt] = useState<string>('');
Â  const [customPromptInput, setCustomPromptInput] = useState<string>('');

Â  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
Â  const chunksRef = useRef<Blob[]>([]);
Â  const videoRef = useRef<HTMLVideoElement>(null);

Â  // For pace/volume interval
Â  const paceIntervalRef = useRef<NodeJS.Timeout | null>(null);
Â  const [lastWordCount, setLastWordCount] = useState(0);

Â  // Pause detection refs
Â  const audioContextRef = useRef<AudioContext | null>(null);
Â  const analyserRef = useRef<AnalyserNode | null>(null);
Â  const silenceTimeoutRef = useRef<NodeJS.Timeout | null>(null);
Â  const startTimeRef = useRef<number>(0);

Â  // Speech recognition refs
Â  const recognitionRef = useRef<any>(null);

Â  // Word Categories
Â  const fillerWords = useMemo(() => ['um', 'uh', 'like', 'you know', 'so', 'actually', 'basically', 'I mean', 'right', 'okay'], []);
Â  const positiveWords = useMemo(() => ['excellent', 'great', 'fantastic', 'confident', 'strong', 'clearly', 'effective', 'beneficial', 'advantage', 'succeeded', 'achieved'], []);
Â  const weakeningWords = useMemo(() => ['just', 'maybe', 'I think', 'sort of', 'kind of', 'perhaps', 'possibly', 'could be', 'might be', 'I guess'], []);

Â  // Highlight all types of words
Â  const highlightTranscript = useCallback((text: string | null) => {
Â  Â  if (!text) return null;

Â  Â  const words = text.split(/\b/);

Â  Â  return words.map((word, index) => {
Â  Â  Â  const lowerCaseWord = word.toLowerCase();
Â  Â  Â  if (fillerWords.includes(lowerCaseWord)) {
Â  Â  Â  Â  return (
Â  Â  Â  Â  Â  <mark key={index} style={{ backgroundColor: '#FFD700', fontWeight: 'bold', borderRadius: '3px', padding: '0 2px' }}>
Â  Â  Â  Â  Â  Â  {word}
Â  Â  Â  Â  Â  </mark>
Â  Â  Â  Â  );
Â  Â  Â  } else if (positiveWords.includes(lowerCaseWord)) {
Â  Â  Â  Â  return (
Â  Â  Â  Â  Â  <mark key={index} style={{ backgroundColor: '#90EE90', fontWeight: 'bold', borderRadius: '3px', padding: '0 2px' }}>
Â  Â  Â  Â  Â  Â  {word}
Â  Â  Â  Â  Â  </mark>
Â  Â  Â  Â  );
Â  Â  Â  } else if (weakeningWords.includes(lowerCaseWord)) {
Â  Â  Â  Â  return (
Â  Â  Â  Â  Â  <mark key={index} style={{ backgroundColor: '#ADD8E6', fontWeight: 'bold', borderRadius: '3px', padding: '0 2px' }}>
Â  Â  Â  Â  Â  Â  {word}
Â  Â  Â  Â  Â  </mark>
Â  Â  Â  Â  );
Â  Â  Â  } else {
Â  Â  Â  Â  return word;
Â  Â  Â  }
Â  Â  });
Â  }, [fillerWords, positiveWords, weakeningWords]);

Â  // Count filler words
Â  const countFillerWords = useCallback((text: string) => {
Â  Â  if (!text) return 0;
Â  Â  const regex = new RegExp(`\\b(${fillerWords.join('|')})\\b`, 'gi');
Â  Â  const matches = text.match(regex);
Â  Â  return matches ? matches.length : 0;
Â  }, [fillerWords]);

Â  // Analyze transcript with Deepseek model
Â  const analyzeTranscript = useCallback(async (text: string) => {
Â  Â  const apiKey = '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';
Â  Â  const url = 'https://openrouter.ai/api/v1/chat/completions';

Â  Â  let prompt = `
Analyze the following transcript of a presentation and provide constructive feedback on these points:
- **Use of filler words or hesitation**: Identify specific instances and suggest alternatives.
- **Clarity and pacing**: Comment on how easy it was to understand the message and the natural flow of speech.
- **Engagement and tone**: Assess the speaker's ability to maintain audience interest and the overall emotional quality of the speech.
- **Use of strong/positive language**: Identify instances where the speaker used confident or impactful language.
- **Use of weakening language**: Point out phrases that might diminish confidence or assertiveness.
- **Overall presentation strengths and areas to improve**: Summarize the key takeaways.
`;

Â  Â  if (practiceMode === 'scenario' && currentPrompt) {
Â  Â  Â  prompt += `

**Consider the following scenario/prompt for your analysis:**
Prompt: "${currentPrompt}"
`;
Â  Â  } else if (practiceMode === 'free') {
Â  Â  Â  prompt += `

**This was a free speech practice session, so provide general feedback.**
`;
Â  Â  }

Â  Â  prompt += `
Transcript:
"""${text}"""
Â  Â  `;

Â  Â  setIsAnalyzing(true);
Â  Â  setAnalysis('');

Â  Â  try {
Â  Â  Â  const response = await fetch(url, {
Â  Â  Â  Â  method: 'POST',
Â  Â  Â  Â  headers: {
Â  Â  Â  Â  Â  'Authorization': `Bearer ${apiKey}`,
Â  Â  Â  Â  Â  'Content-Type': 'application/json',
Â  Â  Â  Â  },
Â  Â  Â  Â  body: JSON.stringify({
Â  Â  Â  Â  Â  model: 'deepseek/deepseek-r1-0528-qwen3-8b:free',
Â  Â  Â  Â  Â  messages: [
Â  Â  Â  Â  Â  Â  {
Â  Â  Â  Â  Â  Â  Â  role: 'user',
Â  Â  Â  Â  Â  Â  Â  content: prompt,
Â  Â  Â  Â  Â  Â  },
Â  Â  Â  Â  Â  ],
Â  Â  Â  Â  }),
Â  Â  Â  });

Â  Â  Â  const data = await response.json();
Â  Â  Â  setAnalysis(data.choices?.[0]?.message?.content ?? 'No analysis received.');
Â  Â  } catch (err) {
Â  Â  Â  console.error('Error analyzing transcript:', err);
Â  Â  Â  setAnalysis('Failed to analyze transcript.');
Â  Â  } finally {
Â  Â  Â  setIsAnalyzing(false);
Â  Â  }
Â  }, [currentPrompt, practiceMode]);

Â  // Set camera stream to video element
Â  useEffect(() => {
Â  Â  if (videoRef.current && cameraStream) {
Â  Â  Â  videoRef.current.srcObject = cameraStream;
Â  Â  }
Â  }, [cameraStream]);

Â  // Cleanup audio URL on component unmount or new recording
Â  useEffect(() => {
Â  Â  return () => {
Â  Â  Â  if (audioURL) {
Â  Â  Â  Â  URL.revokeObjectURL(audioURL);
Â  Â  Â  }
Â  Â  Â  if (paceIntervalRef.current) clearInterval(paceIntervalRef.current);
Â  Â  };
Â  }, [audioURL]);

Â  // Start recording: audio for recording, video for preview only
Â  const startRecording = async () => {
Â  Â  try {
Â  Â  Â  setPauses([]);
Â  Â  Â  setLiveTranscript('');
Â  Â  Â  setTranscript('');
Â  Â  Â  setAnalysis('');
Â  Â  Â  setPaceData([]);
Â  Â  Â  setVolumeData([]);
Â  Â  Â  setLastWordCount(0);
Â  Â  Â  setAudioURL(null);

Â  Â  Â  const audioStream = await navigator.mediaDevices.getUserMedia({ audio: true });
Â  Â  Â  const videoStream = await navigator.mediaDevices.getUserMedia({ video: true });

Â  Â  Â  setCameraStream(videoStream);

Â  Â  Â  // --- Pause Detection Setup ---
Â  Â  Â  audioContextRef.current = new window.AudioContext();
Â  Â  Â  const source = audioContextRef.current.createMediaStreamSource(audioStream);
Â  Â  Â  analyserRef.current = audioContextRef.current.createAnalyser();
Â  Â  Â  source.connect(analyserRef.current);
Â  Â  Â  analyserRef.current.fftSize = 2048;
Â  Â  Â  const dataArray = new Uint8Array(analyserRef.current.fftSize);

Â  Â  Â  let silenceStart: number | null = null;
Â  Â  Â  let lastPauseEnd = 0;
Â  Â  Â  startTimeRef.current = audioContextRef.current.currentTime;

Â  Â  Â  let lastRms = 0;

Â  Â  Â  const checkSilence = () => {
Â  Â  Â  Â  if (!analyserRef.current || !audioContextRef.current) return;
Â  Â  Â  Â  analyserRef.current.getByteTimeDomainData(dataArray);
Â  Â  Â  Â  let sumSquares = 0;
Â  Â  Â  Â  for (let i = 0; i < dataArray.length; i++) {
Â  Â  Â  Â  Â  const normalized = (dataArray[i] - 128) / 128;
Â  Â  Â  Â  Â  sumSquares += normalized * normalized;
Â  Â  Â  Â  }
Â  Â  Â  Â  const rms = Math.sqrt(sumSquares / dataArray.length);
Â  Â  Â  Â  lastRms = rms;

Â  Â  Â  Â  const now = audioContextRef.current.currentTime - startTimeRef.current;
Â  Â  Â  Â  const silenceThreshold = 0.02;
Â  Â  Â  Â  const minSilenceDuration = 0.7;

Â  Â  Â  Â  if (rms < silenceThreshold) {
Â  Â  Â  Â  Â  if (silenceStart === null) {
Â  Â  Â  Â  Â  Â  silenceStart = now;
Â  Â  Â  Â  Â  } else if (now - silenceStart > minSilenceDuration && lastPauseEnd < silenceStart) {
Â  Â  Â  Â  Â  Â  if (typeof silenceStart === 'number' && typeof now === 'number') {
Â  Â  Â  Â  Â  Â  Â  setPauses(prev => [...prev, { start: silenceStart, end: now }]);
Â  Â  Â  Â  Â  Â  Â  lastPauseEnd = now;
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  silenceStart = null;
Â  Â  Â  Â  Â  }
Â  Â  Â  Â  } else {
Â  Â  Â  Â  Â  silenceStart = null;
Â  Â  Â  Â  }
Â  Â  Â  Â  silenceTimeoutRef.current = setTimeout(checkSilence, 100);
Â  Â  Â  };
Â  Â  Â  checkSilence();

Â  Â  Â  // --- Real-Time Transcription Setup ---
Â  Â  Â  // @ts-ignore
Â  Â  Â  const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
Â  Â  Â  if (SpeechRecognition) {
Â  Â  Â  Â  const recognition = new SpeechRecognition();
Â  Â  Â  Â  recognition.lang = 'en-US';
Â  Â  Â  Â  recognition.interimResults = true;
Â  Â  Â  Â  recognition.continuous = true;

Â  Â  Â  Â  recognition.onresult = (event: any) => {
Â  Â  Â  Â  Â  let finalTranscript = '';
Â  Â  Â  Â  Â  let interimTranscript = '';
Â  Â  Â  Â  Â  for (let i = event.resultIndex; i < event.results.length; ++i) {
Â  Â  Â  Â  Â  Â  const transcriptPiece = event.results[i][0].transcript;
Â  Â  Â  Â  Â  Â  if (event.results[i].isFinal) {
Â  Â  Â  Â  Â  Â  Â  finalTranscript += transcriptPiece + ' ';
Â  Â  Â  Â  Â  Â  } else {
Â  Â  Â  Â  Â  Â  Â  interimTranscript += transcriptPiece;
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  setLiveTranscript(finalTranscript + interimTranscript);
Â  Â  Â  Â  };

Â  Â  Â  Â  recognition.onerror = (event: any) => {
Â  Â  Â  Â  Â  console.error('Speech recognition error', event);
Â  Â  Â  Â  };

Â  Â  Â  Â  recognitionRef.current = recognition;
Â  Â  Â  Â  recognition.start();
Â  Â  Â  }

Â  Â  Â  // --- Pace & Volume Data Collection ---
Â  Â  Â  paceIntervalRef.current = setInterval(() => {
Â  Â  Â  Â  if (!audioContextRef.current) return;
Â  Â  Â  Â  const now = audioContextRef.current.currentTime - startTimeRef.current;
Â  Â  Â  Â  const words = liveTranscript.trim().split(/\s+/).filter(Boolean).length;
Â  Â  Â  Â  const currentWordsInInterval = words - lastWordCount;
Â  Â  Â  Â  const wpm = currentWordsInInterval * 120;

Â  Â  Â  Â  setLastWordCount(words);

Â  Â  Â  Â  setPaceData(prev => [...prev, { time: now, wpm: wpm > 0 ? wpm : 0 }]);
Â  Â  Â  Â  setVolumeData(prev => [...prev, { time: now, rms: lastRms }]);
Â  Â  Â  }, 500);

Â  Â  Â  // --- Audio Recording Setup ---
Â  Â  Â  const mediaRecorder = new MediaRecorder(audioStream);
Â  Â  Â  chunksRef.current = [];

Â  Â  Â  mediaRecorder.ondataavailable = (e: BlobEvent) => {
Â  Â  Â  Â  chunksRef.current.push(e.data);
Â  Â  Â  };

Â  Â  Â  mediaRecorder.onstop = async () => {
Â  Â  Â  Â  videoStream.getTracks().forEach(track => track.stop());
Â  Â  Â  Â  setCameraStream(null);

Â  Â  Â  Â  if (audioContextRef.current) {
Â  Â  Â  Â  Â  audioContextRef.current.close();
Â  Â  Â  Â  Â  audioContextRef.current = null;
Â  Â  Â  Â  }
Â  Â  Â  Â  if (silenceTimeoutRef.current) {
Â  Â  Â  Â  Â  clearTimeout(silenceTimeoutRef.current);
Â  Â  Â  Â  }
Â  Â  Â  Â  if (paceIntervalRef.current) {
Â  Â  Â  Â  Â  clearInterval(paceIntervalRef.current);
Â  Â  Â  Â  }

Â  Â  Â  Â  if (recognitionRef.current) {
Â  Â  Â  Â  Â  recognitionRef.current.stop();
Â  Â  Â  Â  }

Â  Â  Â  Â  const blob = new Blob(chunksRef.current, { type: 'audio/webm' });
Â  Â  Â  Â  const url = URL.createObjectURL(blob);
Â  Â  Â  Â  setAudioURL(url);

Â  Â  Â  Â  const formData = new FormData();
Â  Â  Â  Â  formData.append('audio', blob, 'recording.webm');

Â  Â  Â  Â  const res = await fetch('http://localhost:3001/transcribe', {
Â  Â  Â  Â  Â  method: 'POST',
Â  Â  Â  Â  Â  body: formData,
Â  Â  Â  Â  });

Â  Â  Â  Â  const data = await res.json();
Â  Â  Â  Â  setTranscript(data.transcript);

Â  Â  Â  Â  await analyzeTranscript(data.transcript);
Â  Â  Â  };

Â  Â  Â  mediaRecorder.start();
Â  Â  Â  mediaRecorderRef.current = mediaRecorder;
Â  Â  Â  setRecording(true);
Â  Â  } catch (error) {
Â  Â  Â  console.error('Error accessing microphone or camera:', error);
Â  Â  Â  alert('Error accessing microphone or camera. Please ensure permissions are granted.');
Â  Â  }
Â  };

Â  const stopRecording = () => {
Â  Â  if (mediaRecorderRef.current) {
Â  Â  Â  mediaRecorderRef.current.stop();
Â  Â  Â  setRecording(false);
Â  Â  }
Â  };

Â  const handleScenarioChange = (e: React.ChangeEvent<HTMLSelectElement>) => {
Â  Â  const selectedScenario = e.target.value;
Â  Â  switch (selectedScenario) {
Â  Â  Â  case 'job_interview':
Â  Â  Â  Â  setCurrentPrompt('Introduce yourself and explain why you are a good fit for a software engineering role at a tech company.');
Â  Â  Â  Â  setCustomPromptInput('');
Â  Â  Â  Â  break;
Â  Â  Â  case 'sales_pitch':
Â  Â  Â  Â  setCurrentPrompt('Deliver a 2-minute pitch for a new productivity app, highlighting its key benefits.');
Â  Â  Â  Â  setCustomPromptInput('');
Â  Â  Â  Â  break;
Â  Â  Â  case 'academic_presentation':
Â  Â  Â  Â  setCurrentPrompt('Present a summary of a recent research paper in your field, including its findings and implications.');
Â  Â  Â  Â  setCustomPromptInput('');
Â  Â  Â  Â  break;
Â  Â  Â  case 'public_speaking':
Â  Â  Â  Â  setCurrentPrompt('Give a short motivational speech on the importance of perseverance.');
Â  Â  Â  Â  setCustomPromptInput('');
Â  Â  Â  Â  break;
Â  Â  Â  case 'custom':
Â  Â  Â  Â  setCurrentPrompt(customPromptInput);
Â  Â  Â  Â  break;
Â  Â  Â  default:
Â  Â  Â  Â  setCurrentPrompt('');
Â  Â  Â  Â  setCustomPromptInput('');
Â  Â  Â  Â  break;
Â  Â  }
Â  };

Â  const handleCustomPromptChange = (e: React.ChangeEvent<HTMLTextAreaElement>) => {
Â  Â  setCustomPromptInput(e.target.value);
Â  Â  setCurrentPrompt(e.target.value);
Â  };

Â  const resetAllStates = () => {
Â  Â  setTranscript('');
Â  Â  setLiveTranscript('');
Â  Â  setAnalysis('');
Â  Â  setIsAnalyzing(false);
Â  Â  setRecording(false);
Â  Â  setPauses([]);
Â  Â  setCameraStream(null);
Â  Â  setAudioURL(null);
Â  Â  setPaceData([]);
Â  Â  setVolumeData([]);
Â  Â  setLastWordCount(0);
Â  Â  setCurrentPrompt('');
Â  Â  setCustomPromptInput('');
Â  };

Â  useEffect(() => {
Â  Â  if (status === "unauthenticated") {
Â  Â  Â  // User not logged in, redirect to landing page (login)
Â  Â  Â  router.push("/"); // or wherever your landing page is
Â  Â  }
Â  }, [status, router]);

  // --- CONDITIONAL RENDERS CAN GO HERE ---
Â  if (status === "loading") {
Â  Â  return <p>Loading...</p>; // or a spinner
Â  }

Â  if (!session) {
Â  Â  // You can return null or a placeholder while redirect happens
Â  Â  return null;
Â  }
Â Â 
Â  return (
<div className="min-h-screen bg-gradient-to-br from-slate-950 via-purple-950 to-indigo-950 relative overflow-hidden">
  {/* Background Elements */}
  <div className="absolute inset-0">
    <div className="absolute inset-0 bg-[url('data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iODAiIGhlaWdodD0iODAiIHZpZXdCb3g9IjAgMCA4MCA4MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48ZGVmcz48cGF0dGVybiBpZD0iZ3JpZCIgd2lkdGg9IjgwIiBoZWlnaHQ9IjgwIiBwYXR0ZXJuVW5pdHM9InVzZXJTcGFjZU9uVXNlIj48cGF0aCBkPSJNIDgwIDAgTCAwIDAgMCA4MCIgZmlsbD0ibm9uZSIgc3Ryb2tlPSJyZ2JhKDI1NSwyNTUsMjU1LDAuMDIpIiBzdHJva2Utd2lkdGg9IjEiLz48L3BhdHRlcm4+PC9kZWZzPjxyZWN0IHdpZHRoPSIxMDAlIiBoZWlnaHQ9IjEwMCUiIGZpbGw9InVybCgjZ3JpZCIvPjwvc3ZnPg==')] opacity-40"></div>
    <div className="absolute top-20 left-20 w-72 h-72 bg-gradient-radial from-cyan-500/10 to-transparent rounded-full blur-3xl animate-pulse"></div>
    <div className="absolute bottom-32 right-32 w-96 h-96 bg-gradient-radial from-purple-500/10 to-transparent rounded-full blur-3xl animate-pulse delay-1000"></div>
    <div className="absolute top-1/2 left-1/2 transform -translate-x-1/2 -translate-y-1/2 w-64 h-64 bg-gradient-radial from-pink-500/10 to-transparent rounded-full blur-3xl animate-pulse delay-500"></div>
  </div>

  {/* Main Content */}
  <div className="relative z-10 max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
    {/* Header Section */}
    <header className="backdrop-blur-xl bg-black/10 border-b border-white/10 sticky top-0 py-8">
      <div className="text-center space-y-4">
        {/* App Logo/Icon */}
        <div className="relative inline-block">
          <div className="w-16 h-16 bg-gradient-to-br from-cyan-400/20 to-purple-500/20 rounded-2xl flex items-center justify-center backdrop-blur-sm border border-white/10 shadow-xl">
            <span className="text-3xl">ðŸ§ </span>
          </div>
          <div className="absolute -inset-3 bg-gradient-to-r from-cyan-400/20 to-purple-400/20 rounded-full blur-xl animate-pulse"></div>
        </div>
        
        {/* App Title */}
        <h1 className="text-4xl md:text-5xl font-black bg-gradient-to-r from-white via-cyan-200 to-purple-300 bg-clip-text text-transparent tracking-tight">
          EchoMentor
        </h1>
        <p className="text-white/70 text-lg font-medium">
          AI-powered communication coach
        </p>

        {/* User Info */}
        <div className="flex items-center justify-center space-x-4">
          {session.user?.image && (
            <img
              src={session.user.image}
              alt="Profile"
              className="w-8 h-8 rounded-full border border-white/20"
            />
          )}
          <span className="text-white/80">
            {session.user?.name || session.user?.email}
          </span>
          <LoginButton />
        </div>
      </div>
    </header>

    {/* Main App Sections */}
    <main className="space-y-8 py-8">
      {/* Control Panel */}
      <section className="bg-black/20 backdrop-blur-md border border-white/10 rounded-xl p-6 space-y-6">
        <h2 className="text-xl font-semibold text-white/90">Practice Settings</h2>
        
        {/* Practice Mode Selection */}
        <div className="space-y-4">
          <PracticeModeSelector
            practiceMode={practiceMode}
            setPracticeMode={setPracticeMode}
            recording={recording}
            resetAllStates={resetAllStates}
          />
          
          {/* Scenario Selection */}
          {practiceMode === 'scenario' && (
            <ScenarioSelector
              currentPrompt={currentPrompt}
              customPromptInput={customPromptInput}
              handleScenarioChange={handleScenarioChange}
              handleCustomPromptChange={handleCustomPromptChange}
              recording={recording}
            />
          )}
        </div>
        
        {/* Recording Controls */}
        <RecordingControls
          recording={recording}
          startRecording={startRecording}
          stopRecording={stopRecording}
          practiceMode={practiceMode}
          currentPrompt={currentPrompt}
          customPromptInput={customPromptInput}
        />
      </section>

      {/* Live Feedback Section */}
      <section className="space-y-6">
        <h2 className="text-xl font-semibold text-white/90">Live Feedback</h2>
        
        <div className="grid md:grid-cols-2 gap-6">
          {/* Camera Preview */}
          {cameraStream && <CameraPreview videoRef={videoRef} />}

          {/* Live Transcript */}
          {recording && (
            <LiveTranscriptDisplay
              liveTranscript={liveTranscript}
              highlightTranscript={highlightTranscript}
            />
          )}
        </div>

        {/* Audio Playback */}
        {audioURL && <AudioPlayback audioURL={audioURL} />}
      </section>

      {/* Analytics Section */}
      {(recording || paceData.length > 0 || volumeData.length > 0) && (
        <section className="bg-gradient-to-br from-emerald-950/50 to-cyan-950/50 border border-white/10 rounded-xl p-6 backdrop-blur-md">
          <div className="flex items-center space-x-3 mb-6">
            <div className="w-2.5 h-2.5 bg-gradient-to-r from-emerald-400 to-teal-500 rounded-full animate-pulse"></div>
            <h2 className="text-xl font-semibold text-white/90">Speech Analytics</h2>
          </div>
          
          <PaceVolumeChart paceData={paceData} volumeData={volumeData} />
          
          <div className="flex justify-center space-x-8 mt-4">
            <div className="flex items-center space-x-2">
              <div className="w-3 h-3 bg-gradient-to-r from-blue-400 to-cyan-400 rounded-full animate-pulse"></div>
              <span className="text-white/80 text-sm">Speech Pace</span>
            </div>
            <div className="flex items-center space-x-2">
              <div className="w-3 h-3 bg-gradient-to-r from-orange-400 to-red-400 rounded-full animate-pulse delay-300"></div>
              <span className="text-white/80 text-sm">Volume Level</span>
            </div>
          </div>
        </section>
      )}

      {/* Results Section */}
      <section className="space-y-6">
        <h2 className="text-xl font-semibold text-white/90">Analysis Results</h2>
        
        <div className="grid md:grid-cols-2 gap-6">
          {/* Transcript */}
          <FinalTranscriptDisplay
            transcript={transcript}
            highlightTranscript={highlightTranscript}
            countFillerWords={countFillerWords}
          />

          {/* Pauses */}
          <DetectedPausesDisplay pauses={pauses} />
        </div>

        {/* AI Analysis */}
        <AnalysisDisplay analysis={analysis} isAnalyzing={isAnalyzing} />
      </section>
    </main>
  </div>
</div>
Â  );
}